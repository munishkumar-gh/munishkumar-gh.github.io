{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of common commands used in most Python projects; created as a quick reference library\n",
    "\n",
    "Github Repo:\n",
    "\n",
    "https://github.com/munishkumar-gh/munishkumar-gh.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<!--TABLE OF CONTENTS-->\n",
    "\n",
    "- [Libraries](#Libraries)\n",
    "- [Functions](#Functions)\n",
    "- [Globals](#Globals)\n",
    "- [Read CSV](#Read-CSV)\n",
    "- [Conclusion](#Conclusion)\n",
    "- [Write out file](#Write-out-file)\n",
    "- [Copy data frame](#Copy-data-frame)\n",
    "- [Create new df, reset index](#Create-new-df,-reset-index)\n",
    "- [Cast to list](#Cast-to-list)\n",
    "- [Cast to date-time](#Cast-to-date-time)\n",
    "- [Check for Element in List](#Check-for-Element-in-List)\n",
    "- [Concatenate 2 df](#Concatenate-2-df)\n",
    "- [Data Explore](#Data-Explore)\n",
    "- [Drop Columns/rows](#Drop-Columns/rows)\n",
    "- [Diff of 2 data frames with identical cols](#Diff-of-2-data-frames-with-identical-cols)\n",
    "- [Groupby](#Groupby)\n",
    "- [% changes in dataframe between rows](#%-changes-in-dataframe-between-rows)\n",
    "- [Set as Index](#Set-as-Index)\n",
    "- [Split Dataframe to arrays](#Split-Dataframe-to-arrays)\n",
    "- [Visualization](#Visualization)\n",
    "      - [Pairplot for EDA](#Pairplot-for-EDA)\n",
    "      - [Single plot Heatmap](#Single-plot-Heatmap)\n",
    "      - [Subplot with Barplot & Violinplot](#Subplot-with-Barplot-&-Violinplot)\n",
    "      - [Histogram](#Histogram)\n",
    "      - [Scatter plot with X-Y line](#Scatter-plot-with-X-Y-line)\n",
    "      - [Residual Plot](#Residual-Plot)\n",
    "- [Normalize (Math)](#Normalize-(Math))\n",
    "- [One hot encoding](#One-hot-encoding)\n",
    "- [Out of Sample Set](#Out-of-Sample-Set)\n",
    "- [Feature Selection](#Feature-Selection)\n",
    "- [sklearn - Train Test Split](#sklearn---Train-Test-Split)\n",
    "- [sklearn - Normalize Features](#sklearn---Normalize-Features)\n",
    "- [Confusion Matrix](#Confusion-Matrix)\n",
    "- [Example of supervised: Logistic](#Example-of-supervised:-Logistic)\n",
    "- [Example of Unsupervised: K-Means](#Example-of-Unsupervised:-K-Means)\n",
    "- [Others](#Others)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Libraries\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.ticker import NullFormatter\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "sns.set()\n",
    "\n",
    "# Sklearn Liraries\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import datetime\n",
    "from datetime import timedelta, date \n",
    "start = time.time()\n",
    "%matplotlib inline\n",
    "\n",
    "# Forces the print statement to show everything and not truncate\n",
    "# np.set_printoptions(threshold=sys.maxsize) \n",
    "print('Libraries imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acb():\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = 'C:/a/b/c/'\n",
    "filename_suffix = 'csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Means read in the ',' as thousand seperator\n",
    "df = pd.read_csv('XYZ.csv', thousands=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 'Completed Process'\n",
    "elapsed = (time.time() - start)\n",
    "print (\"%s in %s seconds\" % (count,elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write out file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_filename = 'Clean_Data_raw'\n",
    "csvs_sht = os.path.join(dir_name, base_filename + \".\" + filename_suffix)\n",
    "df.to_csv(csvs_sht, index = True, header = True)\n",
    "print (\"Final File Extract Produced:\", base_filename + \".\" + filename_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "df2 = df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new df, reset index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df['a']!='Not assigned']\n",
    "df2 = df2.rename_axis('b').reset_index()\n",
    "\n",
    "df = raw.filter(regex='ab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cast to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = df['XYZ'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cast to date-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Mth'] = pd.to_datetime(df['Mth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for Element in List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = {\"a\", \"b\", \"c\", \"d\"}\n",
    "df[df['XYZ'].isin(lst)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate 2 df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = pd.concat([df2,df1], ignore_index=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Explore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0:5]\n",
    "df.head()\n",
    "df.tail(5)\n",
    "df.info()\n",
    "df.describe(include='all')\n",
    "df.columns.values # Column Names\n",
    "df.shape\n",
    "\n",
    "df['a'].mean()\n",
    "df['a'].unique()\n",
    "\n",
    "df.count()\n",
    "df.sum()\n",
    "df.isnull()\n",
    "df.values\n",
    "df.any()\n",
    "\n",
    "df.sort_values(['a'], inplace = True)\n",
    "df2 = df.sort_values(by='a',axis = 0, ascending=True)\n",
    "\n",
    "col={'a': 'b', 'c':'d'}\n",
    "df.rename(columns=col, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print('The dataframe has {} uniques.'.format(len(df['a'].unique())))\n",
    "## print(\"Delta:\", raw.shape[0]-proc.shape[0])\n",
    "    \n",
    "## Missing Values - df.isnull().values.any()\n",
    "## Total Data Points - df.count().sum()\n",
    "## Total number of missing values - df.isnull().sum().sum()\n",
    "## Outlier - (df_pre_proc == 0).astype(int).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to \n",
    "http://localhost:8888/notebooks/Anaconda3/%40Completed/Weather/3_Pre_Processing_Gov_Data_Part_2.ipynb\n",
    "to infill missing data or 0 data using a normal dist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Columns/rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.dropna(axis = 0) #rows, axis = 1 for col\n",
    "\n",
    "cols = ['a', 'b', 'c']\n",
    "df = df.drop(cols, axis = 1)\n",
    "\n",
    "df = df.loc[:, ~df.columns.str.contains('XyZ')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diff of 2 data frames with identical cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df1.sub(df2.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(df['a']).sum()\n",
    "df.groupby(df['a']).count()\n",
    "\n",
    "df.groupby('a')['b'].transform('sum')\n",
    "df.groupby('a')['b'].transform('count')>5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# % changes in dataframe between rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.pct_change(fill_method ='ffill') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set as Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Multiindex\n",
    "df.index = [f'{y}-{m}' for y, m in df.index] #Assume Date-time\n",
    "df = df.rename_axis('month').reset_index()\n",
    "\n",
    "df.set_index(\"A\", inplace = True)\n",
    "\n",
    "# Change Frequency only for time series\n",
    "df  = df.asfreq('MS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Dataframe to arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a, df_b = np.array_split(data, 2, axis=1) # Split along col into 2 array\n",
    "df = df_a.melt(var_name='groups', value_name='vals') # repeat for df_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pairplot for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette(\"bright\")\n",
    "sns.pairplot(df[['a', 'b', 'c', 'd']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single plot Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "ax = sns.heatmap(df.corr(), annot=True, fmt=\".2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subplot with Barplot & Violinplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Height=10\n",
    "Width=10\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=cols)\n",
    "fig.set_figwidth(width)\n",
    "fig.set_figheight(height)\n",
    "sns.barplot(x=\"groups\", y=\"vals\", data=df, ax=axes[0]) #Refer to 'Split Dataframe to arrays'\n",
    "\n",
    "sns.violinplot(x=\"groups\", y=\"vals\", data=df, ax=axes[0])\n",
    "\n",
    "#Loop labels\n",
    "for i in range(nrows):\n",
    "    ax = axes[i]\n",
    "    if i == 0:\n",
    "        ax.set_title('Violin Plots - Delta Diff')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        continue\n",
    "    if i == 1:\n",
    "        ax.set_ylabel('Frequency')\n",
    "        continue\n",
    "    if i == 2:\n",
    "        ax.set_xlabel('Categories')\n",
    "        ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['a'], ax=axs[0])\n",
    "\n",
    "# Line\n",
    "sns.lineplot(x=df['b'], y=df['a'], color='black', ax=axs[1])\n",
    "plt.title('XXX')\n",
    "plt.xlabel('YYY')\n",
    "plt.ylabel('ZZZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter plot with X-Y line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RedFunction = y_test # Test Data Set\n",
    "BlueFunction = yhat_MLR # Predicted Data Set\n",
    "\n",
    "plt.scatter(RedFunction, BlueFunction, label=\"Test vs Predicted\", s=25, c = color3, zorder=10)\n",
    "xymax = round(max(RedFunction.max()+5, BlueFunction.max()+5), -1)\n",
    "lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
    "]\n",
    "plt.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
    "#plt.axes().set_aspect('equal')\n",
    "plt.xlim(0, xymax)\n",
    "plt.ylim(0, xymax)\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = sns.cubehelix_palette(dark=.9, light=.1, as_cmap=True)\n",
    "sns.residplot(RedFunction, BlueFunction, ax=ax, label = \"Residual\",\n",
    "                  scatter_kws={\"cmap\": cmap}, lowess=True, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize (Math)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = (df-df.min())/(df.max()-df.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onehot = pd.get_dummies(df[['XYZ']], prefix=\"\", prefix_sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out of Sample Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a mask where values that are true go into the training/test set\n",
    "# Note that I done it so that the random number is predictable\n",
    "msk = np.random.seed(0)\n",
    "msk = np.random.rand(len(df))<0.8\n",
    "\n",
    "raw_train_test_set = df[msk]\n",
    "raw_validate_set = df[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature = df[['a', 'b',]]\n",
    "x=Feature\n",
    "\n",
    "y = df['c'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn - Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = 42\n",
    "test_size = 0.2\n",
    "\n",
    "x_train, x_test, y_train, y_test  = train_test_split(\n",
    "            x, y, test_size = test_size, random_state = random_state\n",
    ")\n",
    "\n",
    "print('Train Set: ', x_train.shape, y_train.shape)\n",
    "print(x_train['a'][0:5])\n",
    "print('Test Set: ', x_test.shape, y_test.shape)\n",
    "print(x_test['a'][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn - Normalize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only on Feature set\n",
    "from sklearn. preprocessing import StandardScaler\n",
    "\n",
    "X_train = preprocessing.StandardScaler().fit(x_train).transform(x_train)\n",
    "X_test = preprocessing.StandardScaler().fit(x_test).transform(x_test)\n",
    "print('Normalized X Training Set: ', X_train[0:5])\n",
    "print('Normalized X Testing Set: ', X_test[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The accuracy score with a logistic regression is ~0.7.\n",
    "#A confusion matrix may be more valuable as it allows me to visualize the algorithm performance:\n",
    "#\n",
    "#TN / True Negative: when a case was negative and predicted negative\n",
    "#TP / True Positive: when a case was positive and predicted positive\n",
    "#FN / False Negative: when a case was positive but predicted negative\n",
    "#FP / False Positive: when a case was negative but predicted positive\n",
    "#Precision = TP/(TP + FP)\n",
    "#Recall = TP/(TP + FN)\n",
    "#F1 Score = 2*(Recall * Precision) / (Recall + Precision)\n",
    "\n",
    "def plot_conf_mat(cnf_matrix, classes, normalize, cmap, width, height):\n",
    "    plt.figure(figsize=(width, height))\n",
    "    if normalize == True:\n",
    "        # np.newaxis - make it as column vector by inserting an axis \n",
    "        # along second dimension\n",
    "        cnf_matrix = cnf_matrix.astype('float')/ cnf_matrix.sum(\n",
    "            axis=1)[:,np.newaxis]\n",
    "        print(\"Normalized Confusion Matrix\")\n",
    "    else:\n",
    "        print(\"Confusion Matrix, non-normalized\")\n",
    "    \n",
    "    #imshow() - creates image from 2D numpy array.\n",
    "    plt.imshow(cnf_matrix, interpolation = 'nearest', cmap=cmap)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks (tick_marks, classes, rotation=45)\n",
    "    plt.yticks (tick_marks, classes)\n",
    "    \n",
    "    fmt = '.2f'if normalize else 'd'\n",
    "    thres = cnf_matrix.max()/2\n",
    "    for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):\n",
    "        plt.text(j, i, format(cnf_matrix[i, j], fmt),\n",
    "                 horizontalalignment='center',\n",
    "                 fontsize=20,\n",
    "                 color = 'yellow' if cnf_matrix[i, j] > thres else 'white'\n",
    "                )\n",
    "    # plt.tight_layout()\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.grid(None)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set normalize to true for the confusion matrix to print normalized values\n",
    "normalize = False\n",
    "cmap = plt.cm.seismic\n",
    "width=10\n",
    "height=width/2\n",
    "\n",
    "plot_conf_mat(conf_mat_LR, \n",
    "              classes=['Wins (0)', \n",
    "                       'Losses (1)'],\n",
    "              normalize=normalize, cmap=cmap, width=width, height=height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of supervised: Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn. linear_model import LogisticRegression\n",
    "from sklearn. metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "C = 0.0001\n",
    "\n",
    "LR = LogisticRegression(C = C, solver = 'liblinear')\n",
    "LR.fit(X_train, y_train)\n",
    "\n",
    "yhat_LR = LR.predict(X_test)\n",
    "\n",
    "# accuracy_score(y_true, y_pred)\n",
    "mean_acc_LR = accuracy_score(y_test, yhat_LR)\n",
    "conf_mat_LR = confusion_matrix(y_test, yhat_LR)\n",
    "\n",
    "print('Logistic Regression')\n",
    "print('==============================================\\n')\n",
    "print(\"True values:\", y_test[0:5].round(1))\n",
    "print(\"Pred values:\", yhat_LR[0:5].round(1))\n",
    "print('\\n')\n",
    "print('Mean Accuracy:', mean_acc_LR)\n",
    "print('\\n')\n",
    "print('F1 Score:\\n',classification_report(y_test, yhat_LR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Unsupervised: K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import k-means from clustering stage\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kclusters = 3\n",
    "# Run K-means\n",
    "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(df)\n",
    "\n",
    "#Check cluster label\n",
    "kmeans.labels_[:10]\n",
    "\n",
    "# add cluster label\n",
    "df2[\"Cluster Label\"] = kmeans.labels_\n",
    "\n",
    "# Combine to Original df\n",
    "df2 = cc2.join(df.set_index(\"XYZ\"), on=\"XYZ\")\n",
    "\n",
    "#sort by cluster labels\n",
    "df2.sort_values(['Cluster Label'], inplace = True)\n",
    "df2.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codes - LoL_Diamond_Ranked_Games\n",
    "1. Logistic Regression\n",
    "2. Decision Tree\n",
    "3. Support Vector Machine\n",
    "4. Random Forest\n",
    "5. Neural Net via Tensorflow\n",
    "\n",
    "http://localhost:8888/notebooks/Anaconda3/%40Completed/LoL_Diamond_Ranked_Games/LoL_Diamond_Ranked_Games.ipynb\n",
    "\n",
    "\n",
    "\n",
    "Codes - 4_Supervised_Classification\n",
    "1. Multilinear Regression\n",
    "2. Polynomial Regression\n",
    "3. Ridge Regression\n",
    "4. K Nearest Neighbours\n",
    "5. Decision Tree\n",
    "6. Support Vector Machine\n",
    "\n",
    "http://localhost:8888/notebooks/Anaconda3/%40Completed/Weather/4_Supervised_Classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
