{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of common commands used in most Python projects; created as a quick reference library\n",
    "\n",
    "Github Repo:\n",
    "\n",
    "https://github.com/munishkumar-gh/munishkumar-gh.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<!--TABLE OF CONTENTS-->\n",
    "\n",
    "- [Libraries](#Libraries)\n",
    "- [Functions](#Functions)\n",
    "- [Globals](#Globals)\n",
    "- [Read CSV](#Read-CSV)\n",
    "- [Conclusion](#Conclusion)\n",
    "- [Write out file](#Write-out-file)\n",
    "- [Copy data frame](#Copy-data-frame)\n",
    "- [Create new df, reset index](#Create-new-df,-reset-index)\n",
    "- [Cast to list](#Cast-to-list)\n",
    "- [Cast to date-time](#Cast-to-date-time)\n",
    "- [Check for Element in List](#Check-for-Element-in-List)\n",
    "- [Concatenate 2 df](#Concatenate-2-df)\n",
    "- [Data Explore](#Data-Explore)\n",
    "- [Drop Columns/rows](#Drop-Columns/rows)\n",
    "- [Diff of 2 data frames with identical cols](#Diff-of-2-data-frames-with-identical-cols)\n",
    "- [Groupby](#Groupby)\n",
    "- [% changes in dataframe between rows](#%-changes-in-dataframe-between-rows)\n",
    "- [Set as Index](#Set-as-Index)\n",
    "- [Split Dataframe to arrays](#Split-Dataframe-to-arrays)\n",
    "- [Visualization](#Visualization)\n",
    "      - [Pairplot for EDA](#Pairplot-for-EDA)\n",
    "      - [Single plot Heatmap](#Single-plot-Heatmap)\n",
    "      - [Subplot with Barplot & Violinplot](#Subplot-with-Barplot-&-Violinplot)\n",
    "      - [Histogram](#Histogram)\n",
    "      - [Scatter plot with X-Y line](#Scatter-plot-with-X-Y-line)\n",
    "      - [Residual Plot](#Residual-Plot)\n",
    "- [Normalize (Math)](#Normalize-(Math))\n",
    "- [One hot encoding](#One-hot-encoding)\n",
    "- [Out of Sample Set](#Out-of-Sample-Set)\n",
    "- [Feature Selection](#Feature-Selection)\n",
    "- [sklearn - Train Test Split](#sklearn---Train-Test-Split)\n",
    "- [sklearn - Normalize Features](#sklearn---Normalize-Features)\n",
    "- [Confusion Matrix](#Confusion-Matrix)\n",
    "- [Example of supervised: Logistic](#Example-of-supervised:-Logistic)\n",
    "- [Example of Unsupervised: K-Means](#Example-of-Unsupervised:-K-Means)\n",
    "- [Others](#Others)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Libraries\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.ticker import NullFormatter\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "sns.set()\n",
    "\n",
    "import datetime\n",
    "from datetime import timedelta, date \n",
    "start = time.time()\n",
    "%matplotlib inline\n",
    "\n",
    "# Forces the print statement to show everything and not truncate\n",
    "# np.set_printoptions(threshold=sys.maxsize) \n",
    "print('Libraries imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only uncomment if none of the following is installed\n",
    "\n",
    "#!conda install -c anaconda scikit-learn --yes\n",
    "#!conda install -c anaconda scipy --yes\n",
    "#!conda install -c anaconda dash --yes\n",
    "#!conda install -c anaconda plotly --yes\n",
    "#!conda install -c anaconda multiprocess --yes\n",
    "#!conda install -c anaconda nltk --yes\n",
    "#!conda install -c https://conda.anaconda.org/conda-forge wordcloud --yes\n",
    "#!conda install -c conda-forge textblob --yes\n",
    "#!conda install -c districtdatalabs yellowbrick --yes\n",
    "\n",
    "# Advanced Libraries \n",
    "# Sklearn & NLP libraries\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer # Preprocessing - Stemming\n",
    "from textblob import TextBlob #spelling corrections\n",
    "from textblob import Word # Preprocessing - Lematization \n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from yellowbrick.text import TSNEVisualizer\n",
    "\n",
    "# Word Embedding + ML libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer  #Word embedding\n",
    "from sklearn import model_selection, linear_model, naive_bayes, ensemble, metrics, preprocessing  # different ML\n",
    "from xgboost import XGBClassifier #ML algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acb():\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To color a cell background\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from IPython.display import Image\n",
    "\n",
    "# To use, just type set_background('XXX') where XXX is whatever colour you want\n",
    "def set_background(color):         \n",
    "    script = (\"var cell = this.closest('.code_cell');\" \n",
    "              \"var editor = cell.querySelector('.input_area');\"         \n",
    "              \"editor.style.background='{}';\"         \n",
    "              \"this.parentNode.removeChild(this)\"     \n",
    "             ).format(color)      \n",
    "    display(HTML('<img src onerror=\"{}\">'.format(script)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = 'C:/a/b/c/'\n",
    "filename_suffix = 'csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Means read in the ',' as thousand seperator\n",
    "df = pd.read_csv('XYZ.csv', thousands=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 'Completed Process'\n",
    "elapsed = (time.time() - start)\n",
    "print (\"%s in %s seconds\" % (count,elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write out file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_filename = 'Clean_Data_raw'\n",
    "csvs_sht = os.path.join(dir_name, base_filename + \".\" + filename_suffix)\n",
    "df.to_csv(csvs_sht, index = True, header = True)\n",
    "print (\"Final File Extract Produced:\", base_filename + \".\" + filename_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "df2 = df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new df, reset index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df['a']!='Not assigned']\n",
    "df2 = df2.rename_axis('b').reset_index()\n",
    "\n",
    "df = raw.filter(regex='ab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cast to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = df['XYZ'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cast to date-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Mth'] = pd.to_datetime(df['Mth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for Element in List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = {\"a\", \"b\", \"c\", \"d\"}\n",
    "df[df['XYZ'].isin(lst)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate 2 df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = pd.concat([df2,df1], ignore_index=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Explore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0:5]\n",
    "df.head()\n",
    "df.tail(5)\n",
    "df.info()\n",
    "df.describe(include='all')\n",
    "df.columns.values # Column Names\n",
    "df.shape\n",
    "\n",
    "df['a'].mean()\n",
    "df['a'].unique()\n",
    "\n",
    "df.count()\n",
    "df.sum()\n",
    "df.isnull()\n",
    "df.values\n",
    "df.any()\n",
    "\n",
    "df.sort_values(['a'], inplace = True)\n",
    "df2 = df.sort_values(by='a',axis = 0, ascending=True)\n",
    "\n",
    "col={'a': 'b', 'c':'d'}\n",
    "df.rename(columns=col, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print('The dataframe has {} uniques.'.format(len(df['a'].unique())))\n",
    "## print(\"Delta:\", raw.shape[0]-proc.shape[0])\n",
    "    \n",
    "## Missing Values - df.isnull().values.any()\n",
    "## Total Data Points - df.count().sum()\n",
    "## Total number of missing values - df.isnull().sum().sum()\n",
    "## Outlier - (df_pre_proc == 0).astype(int).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to \n",
    "http://localhost:8888/notebooks/Anaconda3/%40Completed/Weather/3_Pre_Processing_Gov_Data_Part_2.ipynb\n",
    "to infill missing data or 0 data using a normal dist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Columns/rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.dropna(axis = 0) #rows, axis = 1 for col\n",
    "\n",
    "cols = ['a', 'b', 'c']\n",
    "df = df.drop(cols, axis = 1)\n",
    "\n",
    "df = df.loc[:, ~df.columns.str.contains('XyZ')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diff of 2 data frames with identical cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df1.sub(df2.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(df['a']).sum()\n",
    "df.groupby(df['a']).count()\n",
    "\n",
    "df.groupby('a')['b'].transform('sum')\n",
    "df.groupby('a')['b'].transform('count')>5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate through columns in Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_result = {}\n",
    "for col in df_preproc.columns.values:  \n",
    "    adf_result[col] = sts.adfuller(df_preproc[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# % changes in dataframe between rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.pct_change(fill_method ='ffill') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set as Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Multiindex\n",
    "df.index = [f'{y}-{m}' for y, m in df.index] #Assume Date-time\n",
    "df = df.rename_axis('month').reset_index()\n",
    "\n",
    "df.set_index(\"A\", inplace = True)\n",
    "\n",
    "# Change Frequency only for time series\n",
    "df  = df.asfreq('MS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Dataframe to arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a, df_b = np.array_split(data, 2, axis=1) # Split along col into 2 array\n",
    "df = df_a.melt(var_name='groups', value_name='vals') # repeat for df_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pairplot for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette(\"bright\")\n",
    "sns.pairplot(df[['a', 'b', 'c', 'd']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single plot Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "ax = sns.heatmap(df.corr(), annot=True, fmt=\".2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subplot with Barplot & Violinplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Height=10\n",
    "Width=10\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=cols)\n",
    "fig.set_figwidth(width)\n",
    "fig.set_figheight(height)\n",
    "sns.barplot(x=\"groups\", y=\"vals\", data=df, ax=axes[0]) #Refer to 'Split Dataframe to arrays'\n",
    "\n",
    "sns.violinplot(x=\"groups\", y=\"vals\", data=df, ax=axes[0])\n",
    "\n",
    "#Loop labels\n",
    "for i in range(nrows):\n",
    "    ax = axes[i]\n",
    "    if i == 0:\n",
    "        ax.set_title('Violin Plots - Delta Diff')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        continue\n",
    "    if i == 1:\n",
    "        ax.set_ylabel('Frequency')\n",
    "        continue\n",
    "    if i == 2:\n",
    "        ax.set_xlabel('Categories')\n",
    "        ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['a'], ax=axs[0])\n",
    "\n",
    "# Line\n",
    "sns.lineplot(x=df['b'], y=df['a'], color='black', ax=axs[1])\n",
    "plt.title('XXX')\n",
    "plt.xlabel('YYY')\n",
    "plt.ylabel('ZZZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter plot with X-Y line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RedFunction = y_test # Test Data Set\n",
    "BlueFunction = yhat_MLR # Predicted Data Set\n",
    "\n",
    "plt.scatter(RedFunction, BlueFunction, label=\"Test vs Predicted\", s=25, c = color3, zorder=10)\n",
    "xymax = round(max(RedFunction.max()+5, BlueFunction.max()+5), -1)\n",
    "lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
    "]\n",
    "plt.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
    "#plt.axes().set_aspect('equal')\n",
    "plt.xlim(0, xymax)\n",
    "plt.ylim(0, xymax)\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = sns.cubehelix_palette(dark=.9, light=.1, as_cmap=True)\n",
    "sns.residplot(RedFunction, BlueFunction, ax=ax, label = \"Residual\",\n",
    "                  scatter_kws={\"cmap\": cmap}, lowess=True, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize (Math)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = (df-df.min())/(df.max()-df.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onehot = pd.get_dummies(df[['XYZ']], prefix=\"\", prefix_sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out of Sample Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a mask where values that are true go into the training/test set\n",
    "# Note that I done it so that the random number is predictable\n",
    "msk = np.random.seed(0)\n",
    "msk = np.random.rand(len(df))<0.8\n",
    "\n",
    "raw_train_test_set = df[msk]\n",
    "raw_validate_set = df[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature = df[['a', 'b',]]\n",
    "x=Feature\n",
    "\n",
    "y = df['c'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn - Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = 42\n",
    "test_size = 0.2\n",
    "\n",
    "x_train, x_test, y_train, y_test  = train_test_split(\n",
    "            x, y, test_size = test_size, random_state = random_state\n",
    ")\n",
    "\n",
    "print('Train Set: ', x_train.shape, y_train.shape)\n",
    "print(x_train['a'][0:5])\n",
    "print('Test Set: ', x_test.shape, y_test.shape)\n",
    "print(x_test['a'][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn - Normalize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only on Feature set\n",
    "from sklearn. preprocessing import StandardScaler\n",
    "\n",
    "X_train = preprocessing.StandardScaler().fit(x_train).transform(x_train)\n",
    "X_test = preprocessing.StandardScaler().fit(x_test).transform(x_test)\n",
    "print('Normalized X Training Set: ', X_train[0:5])\n",
    "print('Normalized X Testing Set: ', X_test[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The accuracy score with a logistic regression is ~0.7.\n",
    "#A confusion matrix may be more valuable as it allows me to visualize the algorithm performance:\n",
    "#\n",
    "#TN / True Negative: when a case was negative and predicted negative\n",
    "#TP / True Positive: when a case was positive and predicted positive\n",
    "#FN / False Negative: when a case was positive but predicted negative\n",
    "#FP / False Positive: when a case was negative but predicted positive\n",
    "#Precision = TP/(TP + FP)\n",
    "#Recall = TP/(TP + FN)\n",
    "#F1 Score = 2*(Recall * Precision) / (Recall + Precision)\n",
    "\n",
    "def plot_conf_mat(cnf_matrix, classes, normalize, cmap, width, height):\n",
    "    plt.figure(figsize=(width, height))\n",
    "    if normalize == True:\n",
    "        # np.newaxis - make it as column vector by inserting an axis \n",
    "        # along second dimension\n",
    "        cnf_matrix = cnf_matrix.astype('float')/ cnf_matrix.sum(\n",
    "            axis=1)[:,np.newaxis]\n",
    "        print(\"Normalized Confusion Matrix\")\n",
    "    else:\n",
    "        print(\"Confusion Matrix, non-normalized\")\n",
    "    \n",
    "    #imshow() - creates image from 2D numpy array.\n",
    "    plt.imshow(cnf_matrix, interpolation = 'nearest', cmap=cmap)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks (tick_marks, classes, rotation=45)\n",
    "    plt.yticks (tick_marks, classes)\n",
    "    \n",
    "    fmt = '.2f'if normalize else 'd'\n",
    "    thres = cnf_matrix.max()/2\n",
    "    for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):\n",
    "        plt.text(j, i, format(cnf_matrix[i, j], fmt),\n",
    "                 horizontalalignment='center',\n",
    "                 fontsize=20,\n",
    "                 color = 'yellow' if cnf_matrix[i, j] > thres else 'white'\n",
    "                )\n",
    "    # plt.tight_layout()\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.grid(None)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set normalize to true for the confusion matrix to print normalized values\n",
    "normalize = False\n",
    "cmap = plt.cm.seismic\n",
    "width=10\n",
    "height=width/2\n",
    "\n",
    "plot_conf_mat(conf_mat_LR, \n",
    "              classes=['Wins (0)', \n",
    "                       'Losses (1)'],\n",
    "              normalize=normalize, cmap=cmap, width=width, height=height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of supervised: Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn. linear_model import LogisticRegression\n",
    "from sklearn. metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "C = 0.0001\n",
    "\n",
    "LR = LogisticRegression(C = C, solver = 'liblinear')\n",
    "LR.fit(X_train, y_train)\n",
    "\n",
    "yhat_LR = LR.predict(X_test)\n",
    "\n",
    "# accuracy_score(y_true, y_pred)\n",
    "mean_acc_LR = accuracy_score(y_test, yhat_LR)\n",
    "conf_mat_LR = confusion_matrix(y_test, yhat_LR)\n",
    "\n",
    "print('Logistic Regression')\n",
    "print('==============================================\\n')\n",
    "print(\"True values:\", y_test[0:5].round(1))\n",
    "print(\"Pred values:\", yhat_LR[0:5].round(1))\n",
    "print('\\n')\n",
    "print('Mean Accuracy:', mean_acc_LR)\n",
    "print('\\n')\n",
    "print('F1 Score:\\n',classification_report(y_test, yhat_LR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Unsupervised: K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import k-means from clustering stage\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kclusters = 3\n",
    "# Run K-means\n",
    "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(df)\n",
    "\n",
    "#Check cluster label\n",
    "kmeans.labels_[:10]\n",
    "\n",
    "# add cluster label\n",
    "df2[\"Cluster Label\"] = kmeans.labels_\n",
    "\n",
    "# Combine to Original df\n",
    "df2 = cc2.join(df.set_index(\"XYZ\"), on=\"XYZ\")\n",
    "\n",
    "#sort by cluster labels\n",
    "df2.sort_values(['Cluster Label'], inplace = True)\n",
    "df2.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codes - LoL_Diamond_Ranked_Games\n",
    "1. Logistic Regression\n",
    "2. Decision Tree\n",
    "3. Support Vector Machine\n",
    "4. Random Forest\n",
    "5. Neural Net via Tensorflow\n",
    "\n",
    "http://localhost:8888/notebooks/Anaconda3/%40Completed/LoL_Diamond_Ranked_Games/LoL_Diamond_Ranked_Games.ipynb\n",
    "\n",
    "\n",
    "\n",
    "Codes - 4_Supervised_Classification\n",
    "1. Multilinear Regression\n",
    "2. Polynomial Regression\n",
    "3. Ridge Regression\n",
    "4. K Nearest Neighbours\n",
    "5. Decision Tree\n",
    "6. Support Vector Machine\n",
    "\n",
    "http://localhost:8888/notebooks/Anaconda3/%40Completed/Weather/4_Supervised_Classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
